✅ Prompt for Project Code Generation
Please follow below guides.

[General Instructions]
    - This is for writing python code to handle pandas dataframe.
    - Use numpy for vectorization.
        - data will be more than 100 million rows.    
    - Use duckdb for local debugging with commented block.
    - Python Version: 3.10.5
[Input Tables]
    - If the specification includes an "Input Tables" section, for example:
        Input Tables (*):
            (Input 1) S/In FCST(GI)_AP1
                df_in_Sin_GI_AP1
    - Then save df_in_Sin_GI_AP1 to input_dataframes like below:
        STR_DF_IN_SIN_GI_AP1 = 'df_in_Sin_GI_AP1'
        input_dataframes[STR_DF_IN_SIN_GI_AP1] = df_in_Sin_GI_AP1
    - In function code, read it like below:
        df_sin = input_dataframes[STR_DF_IN_SIN_GI_AP1]
    - Do NOT use the global keyword inside functions. 
        my python code will be plugined to o9 Server. in o9, when I use global keyword , o9 will raise error.
[Output Tables]
    - If the specification includes an "Output Tables" section, for example:
    - Output Tables (*):
        (Output 1) df_output_Dummy_EstimatedPrice_USD
    - Then return the function result and assign it in main:
        STR_DF_OUTPUT_DUMMY_ESTIMATEDPRICE_USD = 'df_output_Dummy_EstimatedPrice_USD'
        df_output_Dummy_EstimatedPrice_USD = fn_output_formatter(Version, **dict_log)
        fn_log_dataframe(df_output_Dummy_EstimatedPrice_USD, STR_DF_OUTPUT_DUMMY_ESTIMATEDPRICE_USD)
        output_dataframes[STR_DF_OUTPUT_DUMMY_ESTIMATEDPRICE_USD] = df_output_Dummy_EstimatedPrice_USD
   
[Description for code]
    - Function to process steps
        - I wish there is enough comment in code.    
        - It would be nice if the comments reflected specification.  
        - below is example for step function comment. belocked by """ """
            @_decoration_
            def step03_adjust_negative_values() -> pd.DataFrame:
                """
                Step 3 (벡터라이즈 버전)
                1)  Step 2 결과(df_step02_allocate) 로드
                2)  (Ship-To, Item, Location) 단위로 주차 오름차순 정렬
                3)  각 그룹을 NumPy 배열로 변환해 **역방향 누적-보정** 알고리즘 수행
                        · 뒤쪽 주차에서 생긴 음수(부족분)를 앞쪽 주차에서 차감
                        · 모든 주차의 S/In FCST(GI)_AP2 ≥ 0 보장
                4)  보정된 값을 DataFrame 에 반영하여 반환
                --------------------------------------------------------------------------
                벡터라이즈 포인트
                • iterrows / loc 루프 제거 –> 그룹 별 NumPy 배열에서만 O(n) 연산
                • 월 5~6 주차 × 수만 그룹까지도 Python 루프 병목 없이 처리
                """
                .... content of function
                return df_step03_adjust_negative_values
        


    - inside of function, It would be nice if you add comment for detailed step like below:   
        - example
            # ── 1) 로드 ───────────────────────────────────────────────────────────────────────────────────────────────────
            df_src = input_dataframes[STR_DF_IN_SIN_GI_AP1]
            
            # ── 2) 그룹별 보정 함수 ────────────────────────────────────────────────────────────────────────────────────────
            def _fix_negative(group: pd.DataFrame) -> pd.DataFrame:
                g = group.sort_values('_WK_INT').copy()
                fcst = g[COL_SIN_FCST_AP2].to_numpy(dtype=int)
                # 역방향 누적-보정 ── O(주차)
                deficit = 0
                for i in range(len(fcst) - 1, -1, -1):          # 뒤→앞
                    val = fcst[i] + deficit
                    if val < 0:
                        deficit = val          # 부족분 carry over
                        fcst[i] = 0
                    else:
                        fcst[i] = val
                        deficit = 0
                g[COL_SIN_FCST_AP2] = fcst
                return g
            # ── 3) 그룹별 적용 (vectorised apply) ──────────────────────────────────────────────────────────────────────────
            key_cols = [COL_SHIP_TO, COL_ITEM, COL_LOC]
            df_fixed = (
                df_src
                .groupby(key_cols, sort=False, group_keys=False)
                .apply(_fix_negative)
                .drop(columns=['_WK_INT'])
            )
            # 정수형 유지
            df_fixed[COL_SIN_FCST_AP2] = df_fixed[COL_SIN_FCST_AP2].astype(int)
            return df_fixed
[Function Naming by Steps]
    - If the spec uses Step1, Step1-1, Step1-2, generate separate functions per sub-step.
    - If a summary names a dataframe like convert_date_to_partial_week, define:
        fn_step01_2_convert_date_to_partial_week(...)
    returning dataframe:
        df_step01_2_convert_date_to_partial_week
[Global Variable Rules]
    - Do NOT use the global keyword in function.
        def function_some():
            # do not use global variable in step function
            # global var_a = ‘some’
    - Declare all results in the main block from function returns.
    - You may assume input_dataframes is globally available.
[input_dataframes , output_dataframes, is_local]
    - this program wiil be plugined to o9 Server. 
    - but in o9, I can not set break point in debug mode .
    - in o9,  there is parameter start with df_in_*, it will be global variable.
        - this global variable is decalred by o9
        - and , in python code, this global variable should not be declared like below.
            df_in_BO_FCST = pd.DataFrame()
            this code initialize df_in_BO_FCST declared by o9  in o9 server.
    - Finally , I will set input tables like df_in_* to input_dataframes.
        - And later, I will read it from input_dataframes.
        - It proceeds regardless of whether it is local or server.
        - and below code is needed.
            if is_local: 
                # 로컬인 경우 Output 폴더를 정리한다.
                for file in os.scandir(str_output_dir):
                    os.remove(file.path)
                # 로컬인 경우 파일을 읽어 입력 변수를 정의한다.
                file_pattern = f"{os.getcwd()}/{str_input_dir}/*.csv" 
                csv_files = glob.glob(file_pattern)
                file_to_df_mapping = {
                    "df_In_FCST(GI)_AP2.csv" :      STR_DF_IN_FCST       ,
                    "df_in_Total_BOD_LT.csv"   :    STR_DF_IN_TOTAL_BOD_LT    
                }
                # Read all CSV files into a dictionary of DataFrames
                for file in csv_files:
                    df = pd.read_csv(file)
                    file_name = file.split("/")[-1].split("\\")[-1].split(".")[0]
                    # df['SourceFile'] = file_name
                    # df.set_index('SourceFile',inplace=True)
                    mapped = False
                    for keyword, frame_name in file_to_df_mapping.items():
                        if file_name.startswith(keyword.split('.')[0]):
                            input_dataframes[frame_name] = df
                            mapped = True
                            break

            else:
                # o9 에서 선언된 변수를 할당한다.
                # 나중에 df_in_SIn_FCST_GI_AP2 를 직접 호출하지 않는다. 
                # df = input_dataframes[STR_DF_IN_FCST]
                input_dataframes[STR_DF_IN_FCST]            = df_in_SIn_FCST_GI_AP2
                input_dataframes[STR_DF_IN_TOTAL_BOD_LT]    = df_in_Forecast_Rule
    
[Variable Naming Conventions]    
    - don't use string directly like 'Version.[Version Name]'. 
        - use constant variable.
        - declare constant in initial area in python program
    - Constants for dataframe names:
        - constants : use capital letters
        - example
            STR_DF_IN_MST_RTS = 'df_in_MST_RTS'
            STR_DF_STEP01_1_RTS_CLEAN = 'df_step01_1_RTS_clean'
    - Constants for column names:
        - constants : use capital letters
        - example
            COL_VERSION    = 'Version.[Version Name]'
            COL_ITEM       = 'Item.[Item]'
            COL_SHIP_TO    = 'Sales Domain.[Ship To]'
    - Constants for colors:
       - constants : use capital letters
       - example
        COLOR_LIGHTBLUE = '10_LIGHTBLUE'
        COLOR_WHITE     = '14_WHITE'
    - If you are requested to modify code, please replace variable name by refering to above naming rule.

[Use Vectorization]
    - Ensure code can handle more than 100 million rows efficiently.
    - don't use iterrows : very slow
        for index, row in df_03_joined_rts_eos.iterrows():
    - don't use "for loop" by using numpy if possible
        - numpy loop is faster than iterrows. but vectorization is more faster. and use vectorization if possible. 
        - example to use numpy loop
            df_src = df_asn_item_week
            n = len(df_src)
            for i in range(n):
                if not valid_ship[i]:
                    continue
                pos   = pos_ship[i]
                pg    = pg_arr[i]

    - don't use merge if possible 
        - even if there is need to see column out of other dataframe , don't use merge
        - use below example ( use df.assign)
            pos  = shipto_idx.get_indexer(df_rts_eos[Sales_Domain_ShipTo].to_numpy())
            lv   = np.where(pos >= 0, level_arr[pos], np.nan)
            df   = df_rts_eos.assign(Item_Lv=lv.astype('int8'))

[Delete Not using object For memory]
    - I have a memory pressure 
    - And If object will be not use any more in later , delete object like below.
        - example 1
            df_asn = df_asn.drop(columns=[Version_Name, Salse_Product_ASN], errors='ignore')
            df_asn['key'] = 1
            df_time['key'] = 1
            df_asn_week = pd.merge(df_asn, df_time, on='key', how='outer').drop('key', axis=1)
            del df_asn, df_time; gc.collect()
        - example 2
            dict_log = {'p_step_no': 70, 'p_step_desc': 'Step 7: Prepare ASN Week Grid'}
            df_asn_week = step07_prepare_asn(input_dataframes[STR_DF_ASN], input_dataframes[STR_DF_TIME], **dict_log)
            fn_log_dataframe(df_asn_week, f'df_07_{STR_DF_ASN_WEEK}')
            del input_dataframes[STR_DF_ASN], input_dataframes[STR_DF_TIME]; gc.collect()

[Date/Week Handling]
    - Do NOT compute weeks by simple integer addition:
        WRONG: int(current_week_normalized) + 7
    - Use provided helper:
        cur_week_plus_7_str = common.gfn_add_week(current_week_normalized, 7)
        cur_week_plus_7 = int(cur_week_plus_7_str)



[DuckDB Validation (Local Debugging)]
    When I hanlding dataframe , I need to check if my code is working well. 
    But sometime data is so big, I have a difficultis to check.
    For checking , I need sometime join. But dataframe does not provide join functions. 
    in duckdb , I can use join. 
    At the end of each function before return , please add code commented block for local debug.
    Because o9 server do not allow duckdb, and I will only use duckdb in local environment for debug.
    
    - example.
        """
            # ═══════════════════════════════════════════════════════
            # EXAMPLE DUCKDB VALIDATION (Step X)
            # ═══════════════════════════════════════════════════════
            import duckdb
            # Handy aliases ↓ – adjust if you renamed anything
            STR_DF_PRE   = STR_DF_STEP02_BAS_13_GREEN      # after Step 2
            STR_DF_POST  = STR_DF_STEP03_2_VD_LEAD         # after Step 3-2
            STR_DF_TAT   = STR_DF_STEP03_1_TAT_MAX         # (GBM,Item) → TATTERM
            STR_DF_ITEM  = STR_DF_IN_ITEM_MASTER           # to display Item GBM
            duckdb.register(STR_DF_PRE,  output_dataframes[STR_DF_PRE])
            duckdb.register(STR_DF_TAT,  output_dataframes[STR_DF_TAT])
            duckdb.register(STR_DF_POST, output_dataframes[STR_DF_POST])
            duckdb.register(STR_DF_ITEM, input_dataframes[STR_DF_ITEM])
            # target for the quick check
            v_item   = 'RF65DG90BDSGTL'
            v_shipto = '300114'
            analysis_query_step03_2 = f'''
            SELECT
                pre['{COL_ITEM}']                     AS item,
                itm['{COL_ITEM_GBM}']                 AS item_gbm,
                pre['{COL_SHIP_TO}']                  AS shipto,
                pre['{COL_TIME_PW}']                  AS week_pw,
                pre['{COL_COLOR}']                    AS colour_before,
                post['{COL_COLOR}']                   AS colour_after,
                pre['{COL_LOCK_COND}']                AS lock_before,
                post['{COL_LOCK_COND}']               AS lock_after,
                tat['{COL_TATTERM}']                  AS tatterm,
                tat['{COL_CW_PLUS_TATTERM}']          AS cw_plus_tatterm,
                {current_week_normalized}             AS curr_week
            FROM   {STR_DF_PRE}  AS pre
            JOIN   {STR_DF_POST} AS post
                ON  pre['{COL_ITEM}']      = post['{COL_ITEM}']
                AND pre['{COL_SHIP_TO}']   = post['{COL_SHIP_TO}']
                AND pre['{COL_TIME_PW}']   = post['{COL_TIME_PW}']
            LEFT  JOIN {STR_DF_TAT}  AS tat
                ON  tat['{COL_ITEM}']      = pre['{COL_ITEM}']
            LEFT  JOIN {STR_DF_ITEM} AS itm
                ON  itm['{COL_ITEM}']      = pre['{COL_ITEM}']
            WHERE  pre['{COL_ITEM}']    = '{v_item}'
            -- AND  pre['{COL_SHIP_TO}'] = '{v_shipto}'
            ORDER  BY week_pw;
            '''
            df_validation_step03_2 = duckdb.query(analysis_query_step03_2).to_df()
            fn_log_dataframe(df_validation_step03_2, 'analysis_result_step03_2')  
        """
        retur df
    
[Main Call Code Format]
    - When calling step function in main code.
        - when you calling , please pass  dict_log like **dict_log.
            before call function , declare dic_log.
            if there is no parameter in step function, just pass only **dict_log 
        - Use below structure :
            - as below code, please add comment for Readability.
            - code example
                if __name__ == '__main__':
                    ################################################################################################################
                    # Step 01-1 – RTS master pre-processing
                    ################################################################################################################
                    dict_log = {
                        'p_step_no': 101,
                        'p_step_desc': 'Step 01-1 – RTS master pre-processing'
                    }
                    df_step01_1_RTS_clean = fn_step01_1_preprocess_rts(**dict_log)
                    fn_log_dataframe(df_step01_1_RTS_clean, STR_DF_STEP01_1_RTS_CLEAN)
                    output_dataframes[STR_DF_STEP01_1_RTS_CLEAN] = df_step01_1_RTS_clean
        - Do NOT add p_df_name to dict_log.
            - Use constant-named variables.
            - Follow consistent naming and structure.
    
[Code Template]	
    - When you make full code, please refer to PYForecastVDPanelMonthInput.py. overal flow is below.
        - package import
        - declare variable
        - utility function
        - main functions
        - main code
            - if __name__ == '__main__':
                - is_local variable
                    - distinguish whether it is running on the developer's personal computer or on the o9 server.
                    - if is_local is True, it means it is a personal developer's environment.
                    - if is_local is False, it means it is running on the o9 server.
                    - code example
                        if is_local:
                            Version = 'CWV_DP'
                            # ----------------------------------------------------
                            # parse_args 대체
                            # input , output 폴더설정. 작업시마다 History를 남기고 싶으면
                            # ----------------------------------------------------
                            input_folder_name  = str_instance
                            output_folder_name = str_instance
                            
                            # ------
                            str_input_dir = f'Input/{input_folder_name}'
                            # ------
                            str_output_dir = f'Output/{output_folder_name}'
                            current_time = datetime.datetime.now()
                            formatted_time = current_time.strftime("%Y%m%d_%H_%M")
                            str_output_dir = f"{str_output_dir}_{formatted_time}"
                            # ------
                            os.makedirs(str_input_dir, exist_ok=True)
                            os.makedirs(str_output_dir, exist_ok=True)
                            
                    - calling main function
                        - declare dict_log
                        - call main function with **dict_log
            - exception handling
            - finally block
